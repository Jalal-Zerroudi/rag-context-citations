{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ee5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Any, Iterable, Optional, Iterator, List ,Tuple\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import hashlib\n",
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "PROJECT_ROOT = Path().resolve()\n",
    "DATA_DIR  = PROJECT_ROOT / \"data\"\n",
    "CACHE_DIR = PROJECT_ROOT / \"cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7015607",
   "metadata": {},
   "source": [
    "### 1 - Chargement de documents (.txt / .pdf) avec m√©tadonn√©es + hash SHA256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3a57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    text: str\n",
    "    meta: Dict[str, Any]\n",
    "\n",
    "def sha256_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def iter_files(data_dir: Path) -> Iterator[Path]:\n",
    "    for p in data_dir.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".txt\", \".pdf\"}:\n",
    "            yield p\n",
    "\n",
    "def load_txt(path: Path) -> List[Document]:\n",
    "    txt = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return [Document(text=txt, meta={\"path\": str(path), \"page\": None})]\n",
    "\n",
    "def load_pdf(path: Path) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    reader = PdfReader(str(path))\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        try:\n",
    "            text = page.extract_text() or \"\"\n",
    "        except Exception:\n",
    "            text = \"\"\n",
    "        if text.strip():\n",
    "            docs.append(Document(text=text, meta={\"path\": str(path), \"page\": i + 1}))\n",
    "    return docs\n",
    "\n",
    "def load_any(path: Path) -> List[Document]:\n",
    "    if path.suffix.lower() == \".txt\":\n",
    "        return load_txt(path)\n",
    "    if path.suffix.lower() == \".pdf\":\n",
    "        return load_pdf(path)\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb1f2f",
   "metadata": {},
   "source": [
    "#### D√©finition (points clairs)\n",
    "\n",
    "* D√©finit une structure `Document` (dataclass) :\n",
    "\n",
    "  * `text` : contenu texte\n",
    "  * `meta` : m√©tadonn√©es (ex: chemin, page)\n",
    "* `sha256_file(path)` :\n",
    "\n",
    "  * Calcule l‚Äôempreinte **SHA-256** d‚Äôun fichier (lecture par chunks 1MB) pour identifier un fichier de fa√ßon unique (utile pour cache/version).\n",
    "* `iter_files(data_dir)` :\n",
    "\n",
    "  * Parcourt r√©cursivement un dossier et renvoie uniquement les fichiers **.txt** et **.pdf**.\n",
    "* `load_txt(path)` :\n",
    "\n",
    "  * Lit un fichier texte en UTF-8 (ignore erreurs) et retourne une liste contenant **un seul Document** (page = `None`).\n",
    "* `load_pdf(path)` :\n",
    "\n",
    "  * Lit un PDF page par page, extrait le texte, et cr√©e un `Document` par page non vide avec `page = i+1`.\n",
    "* `load_any(path)` :\n",
    "\n",
    "  * Routeur simple : appelle `load_txt` ou `load_pdf` selon l‚Äôextension, sinon renvoie une liste vide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eaedee",
   "metadata": {},
   "source": [
    "### 2 - Construction d‚Äôun index RAG (chunking + embeddings + cache + FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627b5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> List[str]:\n",
    "    text = \" \".join(text.split())\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    text: str\n",
    "    meta: Dict[str, Any]  # path, page, chunk_id\n",
    "\n",
    "@dataclass\n",
    "class RAGIndex:\n",
    "    index: faiss.Index\n",
    "    chunks: List[Chunk]         # position i -> chunk\n",
    "    dim: int\n",
    "\n",
    "def _ensure_dirs(cache_dir: Path):\n",
    "    (cache_dir / \"chunks\").mkdir(parents=True, exist_ok=True)\n",
    "    (cache_dir / \"embeddings\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_or_load_index(\n",
    "    data_dir: Path,\n",
    "    cache_dir: Path,\n",
    "    embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    chunk_size: int = 900,\n",
    "    overlap: int = 150,\n",
    ") -> RAGIndex:\n",
    "    \"\"\"\n",
    "    Cache par fichier:\n",
    "    - cache/chunks/<hash>.jsonl\n",
    "    - cache/embeddings/<hash>.npy\n",
    "    + cache/file_hashes.json (mapping path -> hash)\n",
    "    On rebuild l'index FAISS √† partir des caches.\n",
    "    \"\"\"\n",
    "    _ensure_dirs(cache_dir)\n",
    "\n",
    "    hashes_path = cache_dir / \"file_hashes.json\"\n",
    "    old_hashes: Dict[str, str] = {}\n",
    "    if hashes_path.exists():\n",
    "        old_hashes = json.loads(hashes_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    new_hashes: Dict[str, str] = {}\n",
    "    files = sorted(list(iter_files(data_dir)))\n",
    "\n",
    "    # Embedder\n",
    "    embedder = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "    all_chunks: List[Chunk] = []\n",
    "    all_embs: List[np.ndarray] = []\n",
    "\n",
    "    for fp in tqdm(files, desc=\"Scan & Cache\"):\n",
    "        file_hash = sha256_file(fp)\n",
    "        new_hashes[str(fp)] = file_hash\n",
    "\n",
    "        chunks_file = cache_dir / \"chunks\" / f\"{file_hash}.jsonl\"\n",
    "        emb_file = cache_dir / \"embeddings\" / f\"{file_hash}.npy\"\n",
    "\n",
    "        need_recompute = (old_hashes.get(str(fp)) != file_hash) or (not chunks_file.exists()) or (not emb_file.exists())\n",
    "\n",
    "        if need_recompute:\n",
    "            docs: List[Document] = load_any(fp)\n",
    "            file_chunks: List[Chunk] = []\n",
    "            for d in docs:\n",
    "                pieces = chunk_text(d.text, chunk_size=chunk_size, overlap=overlap)\n",
    "                for j, ch in enumerate(pieces):\n",
    "                    meta = dict(d.meta)\n",
    "                    meta[\"chunk_id\"] = j\n",
    "                    file_chunks.append(Chunk(text=ch, meta=meta))\n",
    "\n",
    "            # save chunks jsonl\n",
    "            with chunks_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                for c in file_chunks:\n",
    "                    f.write(json.dumps({\"text\": c.text, \"meta\": c.meta}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            # embed + normalize for cosine\n",
    "            if file_chunks:\n",
    "                embs = embedder.encode([c.text for c in file_chunks], show_progress_bar=False, convert_to_numpy=True)\n",
    "                embs = embs.astype(\"float32\")\n",
    "                faiss.normalize_L2(embs)\n",
    "            else:\n",
    "                embs = np.zeros((0, embedder.get_sentence_embedding_dimension()), dtype=\"float32\")\n",
    "\n",
    "            np.save(emb_file, embs)\n",
    "        # load cached\n",
    "        file_chunks_loaded: List[Chunk] = []\n",
    "        with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                file_chunks_loaded.append(Chunk(text=obj[\"text\"], meta=obj[\"meta\"]))\n",
    "        embs_loaded = np.load(emb_file).astype(\"float32\")\n",
    "\n",
    "        # align safety\n",
    "        if len(file_chunks_loaded) != embs_loaded.shape[0]:\n",
    "            # fallback: recompute quickly (rare)\n",
    "            docs = load_any(fp)\n",
    "            file_chunks_loaded = []\n",
    "            for d in docs:\n",
    "                pieces = chunk_text(d.text, chunk_size=chunk_size, overlap=overlap)\n",
    "                for j, ch in enumerate(pieces):\n",
    "                    meta = dict(d.meta)\n",
    "                    meta[\"chunk_id\"] = j\n",
    "                    file_chunks_loaded.append(Chunk(text=ch, meta=meta))\n",
    "            if file_chunks_loaded:\n",
    "                embs_loaded = embedder.encode([c.text for c in file_chunks_loaded], show_progress_bar=False, convert_to_numpy=True).astype(\"float32\")\n",
    "                faiss.normalize_L2(embs_loaded)\n",
    "            else:\n",
    "                embs_loaded = np.zeros((0, embedder.get_sentence_embedding_dimension()), dtype=\"float32\")\n",
    "\n",
    "        all_chunks.extend(file_chunks_loaded)\n",
    "        if embs_loaded.size:\n",
    "            all_embs.append(embs_loaded)\n",
    "\n",
    "    # save hashes\n",
    "    hashes_path.write_text(json.dumps(new_hashes, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    dim = embedder.get_sentence_embedding_dimension()\n",
    "    if all_embs:\n",
    "        mat = np.vstack(all_embs).astype(\"float32\")\n",
    "    else:\n",
    "        mat = np.zeros((0, dim), dtype=\"float32\")\n",
    "\n",
    "    # FAISS cosine (IP sur vecteurs normalis√©s)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    if mat.shape[0] > 0:\n",
    "        index.add(mat)\n",
    "\n",
    "    return RAGIndex(index=index, chunks=all_chunks, dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607364c",
   "metadata": {},
   "source": [
    "#### D√©finition (points clairs)\n",
    "\n",
    "* `chunk_text(...)` :\n",
    "\n",
    "  * Nettoie le texte (espaces) puis le d√©coupe en **chunks** de taille `chunk_size` avec **overlap** `overlap` (pour garder du contexte entre morceaux).\n",
    "* D√©finit 2 structures :\n",
    "\n",
    "  * `Chunk` : un morceau de texte + m√©tadonn√©es (`path`, `page`, `chunk_id`)\n",
    "  * `RAGIndex` : l‚Äôindex FAISS + la liste des chunks (align√©s par position) + dimension des embeddings\n",
    "* `_ensure_dirs(cache_dir)` :\n",
    "\n",
    "  * Cr√©e les dossiers cache `cache/chunks` et `cache/embeddings`.\n",
    "* `build_or_load_index(...)` :\n",
    "\n",
    "  * Parcourt tous les fichiers `.txt` et `.pdf` dans `data_dir`.\n",
    "  * Calcule un **hash SHA256** par fichier pour d√©tecter les changements.\n",
    "  * Utilise un mod√®le `SentenceTransformer` pour g√©n√©rer des **embeddings** de chaque chunk.\n",
    "  * **Met en cache** par fichier :\n",
    "\n",
    "    * `chunks/<hash>.jsonl` (texte + meta)\n",
    "    * `embeddings/<hash>.npy` (matrice embeddings)\n",
    "    * `file_hashes.json` (mapping path ‚Üí hash)\n",
    "  * Recharge depuis le cache si rien n‚Äôa chang√©, sinon **recalcule** chunks + embeddings.\n",
    "  * Normalise les embeddings (`faiss.normalize_L2`) pour faire une similarit√© **cosine**.\n",
    "  * Construit un index FAISS `IndexFlatIP` (produit scalaire sur vecteurs normalis√©s = cosine) et ajoute tous les embeddings.\n",
    "  * Retourne un objet `RAGIndex` pr√™t pour la recherche.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc1381",
   "metadata": {},
   "source": [
    "### 3 - Recherche RAG : embedding de la requ√™te + top-k chunks via FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "152f2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Retrieved:\n",
    "    chunk: Chunk\n",
    "    score: float\n",
    "    ref_id: int\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, rag_index: RAGIndex, embedder_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.rag_index = rag_index\n",
    "        self.embedder = SentenceTransformer(embedder_name)\n",
    "\n",
    "    def search(self, query: str, topk: int = 6) -> List[Retrieved]:\n",
    "        if self.rag_index.index.ntotal == 0:\n",
    "            return []\n",
    "        q = self.embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "        faiss.normalize_L2(q)\n",
    "        scores, ids = self.rag_index.index.search(q, topk)\n",
    "        out: List[Retrieved] = []\n",
    "        for rank, (idx, sc) in enumerate(zip(ids[0].tolist(), scores[0].tolist()), start=1):\n",
    "            if idx < 0 or idx >= len(self.rag_index.chunks):\n",
    "                continue\n",
    "            out.append(Retrieved(chunk=self.rag_index.chunks[idx], score=float(sc), ref_id=rank))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fbf1f",
   "metadata": {},
   "source": [
    "#### D√©finition (points clairs)\n",
    "\n",
    "* D√©finit `Retrieved` :\n",
    "\n",
    "  * `chunk` : le chunk r√©cup√©r√©\n",
    "  * `score` : score de similarit√© (cosine via FAISS IP sur vecteurs normalis√©s)\n",
    "  * `ref_id` : num√©ro de r√©f√©rence (rang 1..topk)\n",
    "* `Retriever.__init__` :\n",
    "\n",
    "  * Re√ßoit un `RAGIndex` (FAISS + chunks) et charge un `SentenceTransformer` pour encoder les requ√™tes.\n",
    "* `search(query, topk=6)` :\n",
    "\n",
    "  * Si l‚Äôindex est vide ‚Üí renvoie `[]`.\n",
    "  * Encode la requ√™te en vecteur, puis normalise (`faiss.normalize_L2`) pour cosine.\n",
    "  * Interroge FAISS (`index.search`) pour obtenir les `topk` meilleurs ids + scores.\n",
    "  * Convertit chaque r√©sultat en objet `Retrieved`, en v√©rifiant que l‚Äôid correspond bien √† un chunk existant.\n",
    "  * Retourne une liste ordonn√©e par rang (ref_id = 1, 2, 3, ‚Ä¶).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f992ac",
   "metadata": {},
   "source": [
    "### 4 - Streaming chat AtlasCloud (style OpenAI) avec cl√© API + parsing des deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df278e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_URL = \"https://api.atlascloud.ai/v1/chat/completions\"\n",
    "\n",
    "def _auth_headers() -> Dict[str, str]:\n",
    "    key = os.getenv(\"ATLASCLOUD_API_KEY\", \"\").strip()\n",
    "    if not key:\n",
    "        raise RuntimeError(\"ATLASCLOUD_API_KEY manquante. Mets-la dans .env\")\n",
    "    return {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {key}\",\n",
    "    }\n",
    "\n",
    "def chat_stream(messages, model: Optional[str] = None, max_tokens: int = 2048, temperature: float = 0.2) -> Iterable[str]:\n",
    "    \"\"\"\n",
    "    Stream type OpenAI: lignes 'data: {...}' + 'data: [DONE]'\n",
    "    Renvoie les morceaux de texte (delta).\n",
    "    \"\"\"\n",
    "    model = model or os.getenv(\"ATLAS_MODEL\", \"openai/gpt-oss-20b\")\n",
    "    payload: Dict[str, Any] = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "\n",
    "    with requests.post(ATLAS_URL, headers=_auth_headers(), json=payload, stream=True, timeout=300) as r:\n",
    "        r.raise_for_status()\n",
    "        for raw in r.iter_lines(decode_unicode=True):\n",
    "            if not raw:\n",
    "                continue\n",
    "            line = raw.strip()\n",
    "            # Plusieurs providers envoient parfois du JSON direct.\n",
    "            if line.startswith(\"data:\"):\n",
    "                data = line[len(\"data:\"):].strip()\n",
    "                if data == \"[DONE]\":\n",
    "                    break\n",
    "                try:\n",
    "                    obj = json.loads(data)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            else:\n",
    "                # fallback si pas \"data:\"\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "            # Format type OpenAI: choices[0].delta.content\n",
    "            try:\n",
    "                delta = obj[\"choices\"][0].get(\"delta\", {})\n",
    "                content = delta.get(\"content\")\n",
    "                if content:\n",
    "                    yield content\n",
    "            except Exception:\n",
    "                # fallback: choices[0].message.content (non-stream)\n",
    "                try:\n",
    "                    content = obj[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    if content:\n",
    "                        yield content\n",
    "                except Exception:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c673e",
   "metadata": {},
   "source": [
    "#### D√©finition (points clairs)\n",
    "\n",
    "* D√©finit l‚Äôendpoint `ATLAS_URL` pour appeler l‚ÄôAPI AtlasCloud (`/chat/completions`).\n",
    "* `_auth_headers()` :\n",
    "\n",
    "  * R√©cup√®re `ATLASCLOUD_API_KEY` depuis l‚Äôenvironnement (`.env`) et construit les headers `Authorization: Bearer ...`.\n",
    "  * Si la cl√© manque ‚Üí l√®ve une erreur claire.\n",
    "* `chat_stream(messages, ...)` :\n",
    "\n",
    "  * Construit un payload compatible OpenAI (`model`, `messages`, `max_tokens`, `temperature`, `stream=True`).\n",
    "  * Envoie une requ√™te `POST` avec `requests.post(..., stream=True)` pour lire la r√©ponse en flux.\n",
    "  * Lit ligne par ligne (`iter_lines`) et g√®re 2 formats possibles :\n",
    "\n",
    "    * lignes qui commencent par `data: {...}` + `data: [DONE]`\n",
    "    * JSON direct sans `data:`\n",
    "  * Extrait le texte g√©n√©r√© principalement depuis :\n",
    "\n",
    "    * `choices[0].delta.content` (stream OpenAI)\n",
    "  * Sinon fallback sur :\n",
    "\n",
    "    * `choices[0].message.content` (r√©ponse non-stream)\n",
    "  * Renvoie progressivement les morceaux de texte via `yield` (g√©n√©rateur).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d8d62",
   "metadata": {},
   "source": [
    "### 5 - Prompts syst√®me : mode RAG strict (extraction) + mode fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b34e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_STRICT = \"\"\"Tu es un assistant RAG en mode EXTRACTION PURE + MISE EN FORME.\n",
    "\n",
    "R√àGLES ABSOLUES:\n",
    "1) Tu dois UNIQUEMENT copier-coller des passages EXACTS pr√©sents dans les SOURCES.\n",
    "2) Interdit: reformuler, expliquer, r√©sumer, compl√©ter, d√©duire ou corriger le texte.\n",
    "3) Chaque extrait doit avoir une citation [1], [2], etc.\n",
    "4) Si aucune information exacte dans les SOURCES ne r√©pond: r√©ponds EXACTEMENT\n",
    "   \"‚ùå Information non disponible dans mes documents.\"\n",
    "5) N'ajoute PAS de section Sources √† la fin (le serveur g√®re √ßa).\n",
    "6) Maximum 6 extraits.\n",
    "\n",
    "FORMAT:\n",
    "- 1 phrase par ligne (court).\n",
    "- Tu peux garder des lignes \"titre:\" si elles existent dans les sources.\n",
    "- Pas de tableaux, pas de guillemets ajout√©s.\n",
    "- Si tu as \"X : - A - B\", mets chaque \"- ...\" sur une nouvelle ligne.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_FALLBACK = \"\"\"Tu es un assistant.\n",
    "- R√©ponds en fran√ßais.\n",
    "- Si des SOURCES sont fournies, cite [1], [2], etc.\n",
    "- Sinon r√©ponds avec connaissances g√©n√©rales.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141292b",
   "metadata": {},
   "source": [
    "#### D√©finition (points clairs)\n",
    "\n",
    "* `SYSTEM_PROMPT_STRICT` :\n",
    "\n",
    "  * Force un mode **RAG ‚Äúextraction pure‚Äù** : uniquement copier-coller du texte pr√©sent dans les **SOURCES**.\n",
    "  * Interdit toute reformulation/explication/r√©sum√©.\n",
    "  * Exige des **citations** `[1]`, `[2]` pour chaque extrait.\n",
    "  * Si rien n‚Äôest trouv√© dans les sources ‚Üí r√©ponse fixe : `‚ùå Information non disponible dans mes documents.`\n",
    "  * Impose un format : **phrases courtes ligne par ligne**, pas de tableaux, max 6 extraits, et transforme les listes `- ...` en lignes s√©par√©es.\n",
    "* `SYSTEM_PROMPT_FALLBACK` :\n",
    "\n",
    "  * Mode normal : r√©pondre en **fran√ßais**.\n",
    "  * Si des sources existent ‚Üí citer `[1]`, `[2]`.\n",
    "  * Sinon ‚Üí r√©pondre avec des connaissances g√©n√©rales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7442e51",
   "metadata": {},
   "source": [
    "### 6 - Pipeline RAG ‚Äúextraction stricte‚Äù : nettoyage, contexte, citations, mise en forme, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345c1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    if any(x in text for x in (\"√É\", \"√Ç\", \"√¢‚Ç¨\", \"√¢‚Ç¨‚Ñ¢\", \"√¢‚Ç¨≈ì\", \"√¢‚Ç¨\")):\n",
    "        try:\n",
    "            fixed = text.encode(\"latin1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
    "            if fixed:\n",
    "                text = fixed\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    text = (text\n",
    "            .replace(\"\\ufeff\", \"\")\n",
    "            .replace(\"\\u200b\", \"\")\n",
    "            )\n",
    "\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def build_context(retrieved) -> str:\n",
    "    if not retrieved:\n",
    "        return \"SOURCES:\\n(Aucune source trouv√©e)\\n\"\n",
    "\n",
    "    lines = [\"SOURCES:\\n\"]\n",
    "    for r in retrieved:\n",
    "        filename = Path(r.chunk.meta.get(\"path\", \"unknown\")).name\n",
    "        page = r.chunk.meta.get(\"page\", None)\n",
    "        page_str = f\"page {page}\" if page else \"toutes pages\"\n",
    "        chunk = clean_text(r.chunk.text)\n",
    "        lines.append(f\"[{r.ref_id}] {filename} ({page_str})\\n{chunk}\\n\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def run_llm(messages, max_tokens: int = 800, temperature: float = 0.0) -> str:\n",
    "    out: List[str] = []\n",
    "    for tok in chat_stream(messages, max_tokens=max_tokens, temperature=temperature):\n",
    "        out.append(tok)\n",
    "    return clean_text(\"\".join(out).strip())\n",
    "\n",
    "\n",
    "CIT_RE = re.compile(r\"\\[(\\d{1,2})\\]\")\n",
    "CIT_END_RE = re.compile(r\"\\[(\\d{1,2})\\]\\s*$\")\n",
    "TITLE_RE = re.compile(r\"^.{2,160}:\\s*$\")\n",
    "QUOTED_LINE_RE = re.compile(r'^\\s*[\"‚Äú](.*?)[\"‚Äù]\\s*(\\[\\d{1,2}\\])\\s*$')\n",
    "\n",
    "\n",
    "def extract_cited_ids(answer: str) -> List[int]:\n",
    "    ids = re.findall(r\"\\[(\\d{1,2})\\]\", answer)\n",
    "    uniq: List[int] = []\n",
    "    for x in ids:\n",
    "        i = int(x)\n",
    "        if i not in uniq:\n",
    "            uniq.append(i)\n",
    "    return uniq\n",
    "\n",
    "\n",
    "def _strip_line_quotes(line: str) -> str:\n",
    "    m = QUOTED_LINE_RE.match(line.strip())\n",
    "    if not m:\n",
    "        return line.strip()\n",
    "    return f\"{m.group(1).strip()} {m.group(2)}\".strip()\n",
    "\n",
    "\n",
    "def _split_colon_dash_to_bullets(text: str) -> str:\n",
    "    text = text.replace(\": -\", \" :\\n- \")\n",
    "    text = re.sub(r\"\\s-\\s(?=[A-Z√â√à√Ä√Ç√é√î√ô√á])\", \"\\n- \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def _norm_for_match(s: str) -> str:\n",
    "    s = clean_text(s)\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"‚Äô\", \"'\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def _remove_trailing_citation(line: str) -> str:\n",
    "    return re.sub(r\"\\s*\\[\\d{1,2}\\]\\s*$\", \"\", line).strip()\n",
    "\n",
    "\n",
    "def _find_best_ref_id_for_line(line_no_cit: str, retrieved) -> Optional[int]:\n",
    "    target = _norm_for_match(line_no_cit)\n",
    "    if not target:\n",
    "        return None\n",
    "\n",
    "    for r in retrieved:\n",
    "        chunk = _norm_for_match(r.chunk.text)\n",
    "        if target in chunk:\n",
    "            return int(r.ref_id)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _ensure_citation(line: str, retrieved, strict: bool) -> Optional[str]:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    if CIT_END_RE.search(line):\n",
    "        return line\n",
    "\n",
    "    base = _remove_trailing_citation(line)\n",
    "    ref_id = _find_best_ref_id_for_line(base, retrieved)\n",
    "    if ref_id is None:\n",
    "        return None if strict else line\n",
    "    return f\"{base} [{ref_id}]\"\n",
    "\n",
    "\n",
    "def normalize_extraction_markdown(raw: str) -> str:\n",
    "    if not raw:\n",
    "        return raw\n",
    "\n",
    "    raw = clean_text(raw)\n",
    "    if raw == \"‚ùå Information non disponible dans mes documents.\":\n",
    "        return raw\n",
    "\n",
    "    raw = _split_colon_dash_to_bullets(raw)\n",
    "\n",
    "    lines = []\n",
    "    for ln in raw.splitlines():\n",
    "        ln2 = _strip_line_quotes(ln)\n",
    "        if ln2.strip():\n",
    "            lines.append(ln2.strip())\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "def structure_nested_markdown(answer: str, retrieved, strict: bool) -> str:\n",
    "    answer = normalize_extraction_markdown(answer)\n",
    "    if answer == \"‚ùå Information non disponible dans mes documents.\":\n",
    "        return answer\n",
    "\n",
    "    lines = [ln.strip() for ln in answer.splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return \"‚ùå Information non disponible dans mes documents.\"\n",
    "\n",
    "    if lines and lines[0].lower().startswith(\"r√©ponse extraite\"):\n",
    "        lines = lines[1:]\n",
    "\n",
    "    blocks: List[Dict[str, Any]] = []\n",
    "    current_title: Optional[str] = None\n",
    "    current_items: List[str] = []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal current_title, current_items\n",
    "        if current_title is None and not current_items:\n",
    "            return\n",
    "        items_ok: List[str] = []\n",
    "        for it in current_items:\n",
    "            it = it.strip()\n",
    "            if it.startswith(\"- \"):\n",
    "                it = it[2:].strip()\n",
    "            it2 = _ensure_citation(it, retrieved, strict=strict)\n",
    "            if it2:\n",
    "                items_ok.append(it2)\n",
    "\n",
    "        if items_ok:\n",
    "            blocks.append({\"title\": current_title, \"items\": items_ok})\n",
    "\n",
    "        current_title = None\n",
    "        current_items = []\n",
    "\n",
    "    for ln in lines:\n",
    "        if TITLE_RE.match(ln) and not CIT_END_RE.search(ln) and not ln.startswith(\"- \"):\n",
    "            flush()\n",
    "            current_title = ln\n",
    "            continue\n",
    "        current_items.append(ln)\n",
    "\n",
    "    flush()\n",
    "\n",
    "    if strict and not blocks:\n",
    "        return \"‚ùå Information non disponible dans mes documents.\"\n",
    "\n",
    "    out: List[str] = [\"**R√©ponse extraite**\"]\n",
    "\n",
    "    for b in blocks:\n",
    "        title = b[\"title\"]\n",
    "        items = b[\"items\"]\n",
    "\n",
    "        if title:\n",
    "            out.append(f\"- **{title}**\")\n",
    "            for it in items:\n",
    "                out.append(f\"  - {it}\")\n",
    "        else:\n",
    "            for it in items:\n",
    "                out.append(f\"- {it}\")\n",
    "\n",
    "    return \"\\n\".join(out).strip()\n",
    "\n",
    "\n",
    "def format_sources_section(retrieved, cited_ids: List[int]) -> str:\n",
    "    if not retrieved:\n",
    "        return \"\\n\\n---\\n\\nüìö **Sources consult√©es**\\n\\n‚ùå Aucune source disponible\\n\"\n",
    "\n",
    "    by_id = {r.ref_id: r for r in retrieved}\n",
    "    ids_cited = [i for i in cited_ids if i in by_id]\n",
    "\n",
    "    lines = [\"\\n\\n---\\n\\nüìö **Sources consult√©es**\\n\"]\n",
    "    if ids_cited:\n",
    "        for i in ids_cited:\n",
    "            r = by_id[i]\n",
    "            filename = Path(r.chunk.meta.get(\"path\", \"unknown\")).name\n",
    "            page = r.chunk.meta.get(\"page\", None)\n",
    "            page_str = f\"page {page}\" if page else \"toutes pages\"\n",
    "            lines.append(f\"- **[{i}]** `{clean_text(filename)}` ({page_str})\")\n",
    "    else:\n",
    "        for r in retrieved:\n",
    "            filename = Path(r.chunk.meta.get(\"path\", \"unknown\")).name\n",
    "            page = r.chunk.meta.get(\"page\", None)\n",
    "            page_str = f\"page {page}\" if page else \"toutes pages\"\n",
    "            lines.append(f\"- **[{r.ref_id}]** `{clean_text(filename)}` ({page_str})\")\n",
    "\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "\n",
    "def answer_with_rag(retriever: Retriever, question: str, topk: int = 6, strict: bool = True) -> Dict[str, Any]:\n",
    "    question = clean_text(question)\n",
    "    retrieved = retriever.search(question, topk=topk)\n",
    "\n",
    "    if strict and not retrieved:\n",
    "        return {\"answer\": \"‚ùå Information non disponible dans mes documents.\", \"retrieved\": []}\n",
    "\n",
    "    context = build_context(retrieved)\n",
    "\n",
    "    user_prompt = f\"\"\" QUESTION: {question}\n",
    "                        {context}\n",
    "\n",
    "                        CONSIGNE:\n",
    "                        - Extrais uniquement des passages EXACTS des SOURCES (copier-coller).\n",
    "                        - 1 phrase par ligne.\n",
    "                        - Chaque ligne doit finir par [id] (ex: [1]).\n",
    "                        - Interdit: reformuler/expliquer.\n",
    "                        - Si tu as \"X : - A - B\", mets chaque \"- ...\" sur une nouvelle ligne.\n",
    "                        - Si rien: \"‚ùå Information non disponible dans mes documents.\"\n",
    "                    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_STRICT if strict else SYSTEM_PROMPT_FALLBACK},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    raw = run_llm(messages, max_tokens=900, temperature=0.0)\n",
    "    answer = structure_nested_markdown(raw, retrieved, strict=strict)\n",
    "\n",
    "    cited = extract_cited_ids(answer)\n",
    "    if strict and answer != \"‚ùå Information non disponible dans mes documents.\" and not cited:\n",
    "        answer = \"‚ùå Information non disponible dans mes documents.\"\n",
    "        cited = []\n",
    "\n",
    "    sources_section = format_sources_section(retrieved, cited)\n",
    "    final_answer = answer + sources_section\n",
    "\n",
    "    return {\"answer\": final_answer, \"retrieved\": retrieved}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80460adb",
   "metadata": {},
   "source": [
    "#### D√©finition (points clairs)\n",
    "\n",
    "* **Nettoyage texte** (`clean_text`) : corrige quelques probl√®mes d‚Äôencodage (latin1‚Üíutf8), supprime caract√®res invisibles, normalise espaces et sauts de ligne.\n",
    "* **Construction du contexte** (`build_context`) : assemble une section `SOURCES:` avec chaque chunk r√©cup√©r√© + infos fichier/page + id `[1]`, `[2]`‚Ä¶\n",
    "* **Appel LLM** (`run_llm`) : consomme le streaming `chat_stream`, concat√®ne les tokens, puis nettoie le r√©sultat.\n",
    "* **Regex utilitaires** : d√©tecte citations `[n]`, titres ‚Äúxxx:‚Äù, lignes entre guillemets, etc.\n",
    "* **Gestion des citations** :\n",
    "\n",
    "  * `extract_cited_ids` r√©cup√®re les ids cit√©s dans la r√©ponse.\n",
    "  * `_ensure_citation` ajoute une citation manquante en cherchant si la ligne existe dans un chunk (matching simple ‚Äúligne ‚äÇ chunk‚Äù).\n",
    "  * En mode `strict`, une ligne sans source trouv√©e peut √™tre supprim√©e.\n",
    "* **Normalisation du markdown extrait** :\n",
    "\n",
    "  * `normalize_extraction_markdown` enl√®ve guillemets, restructure les listes du type `X : - A - B` en puces sur plusieurs lignes.\n",
    "  * `structure_nested_markdown` regroupe en blocs : titres en **gras** + sous-puces, et garantit (autant que possible) une citation par ligne.\n",
    "* **Section ‚ÄúSources consult√©es‚Äù** (`format_sources_section`) :\n",
    "\n",
    "  * Affiche uniquement les sources r√©ellement cit√©es si possible, sinon liste toutes les sources r√©cup√©r√©es.\n",
    "* **Fonction principale** (`answer_with_rag`) :\n",
    "\n",
    "  * Nettoie la question ‚Üí `retriever.search(topk)`\n",
    "  * Si `strict` et rien trouv√© ‚Üí renvoie directement `‚ùå Information non disponible...`\n",
    "  * Construit un prompt utilisateur (question + SOURCES + consignes strictes)\n",
    "  * Appelle le LLM, restructure la r√©ponse, v√©rifie qu‚Äôil y a des citations (sinon refuse en strict)\n",
    "  * Ajoute la section sources et renvoie `{ \"answer\": ..., \"retrieved\": ... }`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042a632",
   "metadata": {},
   "source": [
    "### 7 - Initialisation RAG : chargement .env + build/load index + cr√©ation du Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6c0294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Chargement index RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scan & Cache: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:02<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index pr√™t.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "print(\"üîÑ Chargement index RAG...\")\n",
    "rag_index = build_or_load_index(data_dir=DATA_DIR, cache_dir=CACHE_DIR)\n",
    "retriever = Retriever(rag_index)\n",
    "print(\"‚úÖ Index pr√™t.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f3715",
   "metadata": {},
   "source": [
    "#### D√©finition (points clairs)\n",
    "\n",
    "* `load_dotenv()` : charge les variables d‚Äôenvironnement depuis le fichier `.env` (ex: cl√© API, mod√®le, etc.).\n",
    "* Construit/charge l‚Äôindex RAG avec `build_or_load_index(DATA_DIR, CACHE_DIR)` (scan fichiers, chunks, embeddings, cache, FAISS).\n",
    "* Cr√©e `retriever = Retriever(rag_index)` pour pouvoir faire des recherches top-k dans l‚Äôindex.\n",
    "* Affiche des messages console pour indiquer le d√©but et la fin du chargement (`Index pr√™t`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3309df8",
   "metadata": {},
   "source": [
    "### 8 - Question RAG : recherche + extraction stricte + affichage de la r√©ponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fec70b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**R√©ponse extraite**\n",
      "- Le Model Context Protocol (MCP) est un protocole standard ouvert qui permet de connecter facilement un mod√®le d‚Äôintelligence artificielle (comme ChatGPT, Claude ou Gemini) √† des outils, des services et des sources de donn√©es externes. [2]\n",
      "- L‚Äôid√©e principale est la suivante : au lieu de cr√©er une int√©gration diff√©rente pour chaque mod√®le d‚ÄôIA et chaque service [2]\n",
      "\n",
      "---\n",
      "\n",
      "üìö **Sources consult√©es**\n",
      "\n",
      "- **[2]** `01_definition_mcp.txt` (toutes pages)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"Le Model Context Protocol ?\"\n",
    "res = answer_with_rag(retriever, q, topk=6, strict=True)\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378b6aa",
   "metadata": {},
   "source": [
    "#### D√©finition (points clairs)\n",
    "\n",
    "* D√©finit la question `q` (‚ÄúLe Model Context Protocol ?‚Äù).\n",
    "* Appelle `answer_with_rag(...)` avec :\n",
    "\n",
    "  * `topk=6` : r√©cup√®re jusqu‚Äô√† 6 chunks les plus proches.\n",
    "  * `strict=True` : r√©ponse **uniquement par copier-coller** des sources, avec citations obligatoires.\n",
    "* R√©cup√®re le texte final dans `res[\"answer\"]` (r√©ponse + ‚ÄúSources consult√©es‚Äù).\n",
    "* Affiche le r√©sultat avec `print(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508f2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Information non disponible dans mes documents.\n",
      "\n",
      "---\n",
      "\n",
      "üìö **Sources consult√©es**\n",
      "\n",
      "- **[1]** `05_exemples_utilisation_mcp.txt` (toutes pages)\n",
      "- **[2]** `06_avantages_limites_mcp.txt` (toutes pages)\n",
      "- **[3]** `01_definition_mcp.txt` (toutes pages)\n",
      "- **[4]** `04_fonctionnement_mcp.txt` (toutes pages)\n",
      "- **[5]** `07_sans_mcp_vs_avec_mcp.txt` (toutes pages)\n",
      "- **[6]** `02_objectifs_mcp.txt` (toutes pages)\n",
      "- **[7]** `06_avantages_limites_mcp.txt` (toutes pages)\n",
      "- **[8]** `07_sans_mcp_vs_avec_mcp.txt` (toutes pages)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"Le nom du roi du Maroc en 2020 ?\"\n",
    "res = answer_with_rag(retriever, q, topk=6, strict=True)\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c0c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
